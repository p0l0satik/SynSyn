{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ccbda8a"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/20357655/145710301-ad00ab66-2378-404f-a918-576aba834ff9.png)"
      ],
      "id": "8ccbda8a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d8c8e89"
      },
      "source": [
        "In this homework, we will try to implement a `View Synthesis` model that allows us to generate new scene views based on a single image.\n",
        "\n",
        "The basic idea is to use `differentiable point cloud rendering`, which is used to convert a hidden 3D feature point cloud into a target view.\n",
        "The projected features are decoded by `refinement network` to inpaint missing regions and generate a realistic output image."
      ],
      "id": "2d8c8e89"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "083c52ab"
      },
      "source": [
        "### Overall pipeline disribed below"
      ],
      "id": "083c52ab"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b90c6e49"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/20357655/145710444-0d0e163f-6996-4eb8-81c0-69798b11c5a6.png)"
      ],
      "id": "b90c6e49"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9712addf"
      },
      "source": [
        "# Data\n",
        "## Download KITTI dataset"
      ],
      "id": "9712addf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26e083a7"
      },
      "outputs": [],
      "source": [
        "! wget https://raw.githubusercontent.com/yandexdataschool/deep_vision_and_graphics/fall21/homework04/gfile.py\n",
        "! wget https://raw.githubusercontent.com/yandexdataschool/deep_vision_and_graphics/fall21/homework04/kitti.py"
      ],
      "id": "26e083a7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bde8b88"
      },
      "source": [
        " ``` python \n",
        " from gfile import download_file_from_google_drive\n",
        " \n",
        " download_file_from_google_drive(\n",
        "    '1lqspXN10biBShBIVD0yvgnl1nIPPhRdC',\n",
        "     'kitti.zip'\n",
        " )\n",
        "```\n",
        "Optionally you can download with native gdown\n",
        "\n",
        "``` sh\n",
        "    gdown --id 1lqspXN10biBShBIVD0yvgnl1nIPPhRdC\n",
        "```"
      ],
      "id": "4bde8b88"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b12756c"
      },
      "outputs": [],
      "source": [
        "# ! unzip dataset_kitti.zip"
      ],
      "id": "1b12756c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eded662"
      },
      "source": [
        "#### pytorch3d installation\n",
        "\n",
        "The pytorch3d library needed to work with geometry. We highly encourage to see on the https://pytorch3d.org/tutorials/\n",
        "\n",
        "To give you hints we provide some snipets, that can work at colab (deends from the overal environment).\n",
        "\n"
      ],
      "id": "4eded662"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a46ca38"
      },
      "outputs": [],
      "source": [
        "# If you version is low enough, you will not have problems\n",
        "# optionally you can downgrade pytorch and use native installation with the following line\n",
        "\n",
        "if torch.__version__.startswith(\"1.11.\"):\n",
        "  !pip3 install torch==1.10.2+cu113  torchvision==0.11.3+cu113  -f https://download.pytorch.org/whl/torch_stable.html\n"
      ],
      "id": "9a46ca38"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d04040f2"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "need_pytorch3d=False\n",
        "try:\n",
        "    import pytorch3d\n",
        "except ModuleNotFoundError:\n",
        "    need_pytorch3d=True\n",
        "if need_pytorch3d:\n",
        "    if torch.__version__.startswith(\"1.10.\") and sys.platform.startswith(\"linux\"):\n",
        "        # We try to install PyTorch3D via a released wheel.\n",
        "        pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
        "        version_str=\"\".join([\n",
        "            f\"py3{sys.version_info.minor}_cu\",\n",
        "            torch.version.cuda.replace(\".\",\"\"),\n",
        "            f\"_pyt{pyt_version_str}\"\n",
        "        ])\n",
        "        !pip install pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
        "    else:\n",
        "        # We try to install PyTorch3D from source.\n",
        "        !curl -LO https://github.com/NVIDIA/cub/archive/1.10.0.tar.gz\n",
        "        !tar xzf 1.10.0.tar.gz\n",
        "        os.environ[\"CUB_HOME\"] = os.getcwd() + \"/cub-1.10.0\"\n",
        "        !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'"
      ],
      "id": "d04040f2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0de1f7dd"
      },
      "source": [
        "## Dataset"
      ],
      "id": "0de1f7dd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7d09277"
      },
      "outputs": [],
      "source": [
        "%pylab inline\n",
        "\n",
        "from tqdm import tqdm\n",
        "from itertools import islice\n",
        "from IPython.display import clear_output, HTML\n",
        "from collections import defaultdict\n",
        "\n",
        "from kitti import KITTIDataLoader\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "\n",
        "from pytorch3d.vis.plotly_vis import plot_scene\n",
        "from pytorch3d.structures import Pointclouds\n",
        "from pytorch3d.renderer import PerspectiveCameras, compositing, rasterize_points"
      ],
      "id": "a7d09277"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a6b9b1f"
      },
      "outputs": [],
      "source": [
        "def split_RT(RT):\n",
        "    return RT[..., :3, :3], RT[..., :3, 3]\n",
        "\n",
        "def renormalize_image(image):\n",
        "    return image * 0.5 + 0.5"
      ],
      "id": "1a6b9b1f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t97AhhRN82WE"
      },
      "outputs": [],
      "source": [
        "#  from gfile import download_file_from_google_drive\n",
        "\n",
        "#  download_file_from_google_drive(\n",
        "#     '1lqspXN10biBShBIVD0yvgnl1nIPPhRdC',\n",
        "#      'kitti.zip'\n",
        "#  )\n",
        "!gdown --id 1lqspXN10biBShBIVD0yvgnl1nIPPhRdC\n",
        "!unzip dataset_kitti.zip\n",
        "# !ls"
      ],
      "id": "t97AhhRN82WE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7cc1f15"
      },
      "outputs": [],
      "source": [
        "dataset = KITTIDataLoader('dataset_kitti')"
      ],
      "id": "f7cc1f15"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a536bbe9"
      },
      "source": [
        "Each instance of dataset contain `source` and `target` images, `extrinsic` and `intrinsic` camera parameters for `source` and `targer` images.\n",
        "\n",
        "It is highly recommended to understand these concepts, e.g., here https://ksimek.github.io/2012/08/22/extrinsic/"
      ],
      "id": "a536bbe9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccd27eca"
      },
      "outputs": [],
      "source": [
        "images, cameras = dataset[0].values()"
      ],
      "id": "ccd27eca"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47657b15"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "ax = plt.subplot(1, 2, 1)\n",
        "ax.imshow(images[0].permute(1, 2, 0) * 0.5 + 0.5)\n",
        "ax.set_title('Source Image Frame', fontsize=20)\n",
        "ax.axis('off')\n",
        "\n",
        "ax = plt.subplot(1, 2, 2)\n",
        "ax.imshow(images[1].permute(1, 2, 0) * 0.5 + 0.5)\n",
        "ax.set_title('Target Image Frame', fontsize=20)\n",
        "ax.axis('off')"
      ],
      "id": "47657b15"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83fdc36a"
      },
      "outputs": [],
      "source": [
        "source_camera = PerspectiveCameras(\n",
        "    R=split_RT(cameras[0]['P'])[0][None],\n",
        "    T=split_RT(cameras[0]['P'])[1][None],\n",
        "    K=torch.from_numpy(cameras[0]['K'])[None]\n",
        ")\n",
        "\n",
        "target_camera = PerspectiveCameras(\n",
        "    R=split_RT(cameras[1]['P'])[0][None],\n",
        "    T=split_RT(cameras[1]['P'])[1][None],\n",
        "    K=torch.from_numpy(cameras[1]['K'])[None]\n",
        ")\n",
        "\n",
        "plot_scene(\n",
        "    {\n",
        "        'scene': {\n",
        "            'source_camera': source_camera,\n",
        "            'target_camera': target_camera\n",
        "        }\n",
        "    },\n",
        ")"
      ],
      "id": "83fdc36a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffb87ce0"
      },
      "outputs": [],
      "source": [
        "indexes = torch.randperm(len(dataset))\n",
        "\n",
        "train_indexes = indexes[:-1000]\n",
        "validation_indexes = indexes[-1000:]\n",
        "\n",
        "train_dataset = Subset(dataset, train_indexes)\n",
        "validation_dataset = Subset(dataset, validation_indexes)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, batch_size=16, num_workers=2,\n",
        "    shuffle=True, drop_last=True, pin_memory=True\n",
        ")\n",
        "validation_dataloder = DataLoader(\n",
        "    validation_dataset, batch_size=10, num_workers=2,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "id": "ffb87ce0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1463727"
      },
      "source": [
        "---"
      ],
      "id": "a1463727"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8b2df13"
      },
      "source": [
        "# Models\n",
        "Our main task here is to understand the simple way to solve novel view synthesis problem from single image.\n",
        "We need to implement `Spatial Feature Predictor`, `Depth Regressor`, `Point Cloud Renderer` and `RefinementNetwork`."
      ],
      "id": "a8b2df13"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e26dacb"
      },
      "source": [
        "One of the main building blocks in these networks is `ResNetBlock`, but with some modifications:\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/20357655/145711537-eebb0cb9-8935-4d65-bc4b-559c1e19ba98.png)\n",
        "\n",
        "So, let's implement it, but without the noise part `Linear + z`"
      ],
      "id": "5e26dacb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6aa09af"
      },
      "outputs": [],
      "source": [
        "class ResNetBlock(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        stride: int = 1,\n",
        "        mode = 'identity'\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.activation = nn.LeakyReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        self.identity_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        \n",
        "        if mode == 'identity':\n",
        "            self.identity_upsample = nn.Identity()\n",
        "        elif mode == 'up':\n",
        "            self.identity_upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        elif mode == 'down':\n",
        "            self.identity_upsample = nn.AvgPool2d(kernel_size=2)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        identity = input\n",
        "\n",
        "        out = self.bn1(input)\n",
        "        out = self.activation(out)\n",
        "        out = self.identity_upsample(out)\n",
        "        out = self.conv1(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.conv2(out)\n",
        "\n",
        "        identity = self.identity_upsample(identity)\n",
        "        identity = self.identity_conv(identity)\n",
        "        out += identity\n",
        "\n",
        "        return out"
      ],
      "id": "d6aa09af"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44866d81"
      },
      "source": [
        "## Spatial Feature Predictor\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/20357655/145711931-be08e4f9-f383-4942-8b93-f8bdfd3060d2.png)"
      ],
      "id": "44866d81"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e76a42f6"
      },
      "outputs": [],
      "source": [
        "class SpatialFeatureNetwork(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels=3, out_channels=64):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.Sequential(\n",
        "            # TODO\n",
        "        )\n",
        "\n",
        "    def forward(self, input: torch.Tensor):\n",
        "        return self.blocks(input)\n",
        "\n",
        "sf_net = SpatialFeatureNetwork()\n",
        "\n",
        "test_in = torch.rand(2, 3, 100, 100) \n",
        "test_out = sf_net(test_in)\n",
        "assert list(test_out.shape) == [2, 64, 100, 100], test_out.shape\n",
        "assert len(sf_net.blocks) == 8, len(sf_net.blocks)"
      ],
      "id": "e76a42f6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "622d6c5c"
      },
      "source": [
        "## Depth Regressor\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/20357655/145711785-690008e5-96d0-418f-adf1-1509e399c92e.png)\n",
        "\n",
        "A green convolution has parameters: stride 2, padding 1, kernel size 4.\n",
        "\n",
        "An `Enc Block` consists of a sequence of Leaky ReLU, convolution (stride 2, padding 1, kernel size 4), and batch normalisation layers.\n",
        "\n",
        "A `Dec Block` consists of a sequence of ReLU, 2x bilinear upsampling, convolution (stride 1, padding 1, kernel size3), and batch normalisation layers (except for the final layer, which has no batch normalisation layer)."
      ],
      "id": "622d6c5c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cd12e4c"
      },
      "outputs": [],
      "source": [
        "from re import X\n",
        "\n",
        "# TODO class EncoderBlock\n",
        "\n",
        "test_input = torch.randn(2, 64, 128, 128)\n",
        "encoder = EncoderBlock(64, 128)\n",
        "assert list(encoder(test_input).shape) == [2, 128, 64, 64], encoder(test_input).shape\n",
        "\n",
        "# TODO class DecoderBlock\n",
        "\n",
        "decoder = DecoderBlock(64, 32)\n",
        "assert list(decoder(test_input).shape) == [2, 32, 256, 256], decoder(test_input).shape\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_filters=32,\n",
        "        channels_in=3,\n",
        "        channels_out=3\n",
        "    ):\n",
        "        super(Unet, self).__init__()\n",
        "        self.encoder = nn.ModuleList([\n",
        "            #TODO\n",
        "        ])\n",
        "        self.decoder = nn.ModuleList([\n",
        "            #TODO\n",
        "            \n",
        "        ])\n",
        "        self.final_layer = DecoderBlock(num_filters, channels_out, batch_norm=False)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, input):\n",
        "      # TODO  x in [-1, 1]\n",
        "      \n",
        "      return x\n",
        "\n",
        "unet = Unet()\n",
        "test_input = torch.randn(2, 3, 512, 512)\n",
        "assert unet(test_input).shape == test_input.shape, unet(test_input).shape"
      ],
      "id": "1cd12e4c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea7025ae"
      },
      "source": [
        "## Refinement Network\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/20357655/145711921-45ebf1e5-e852-4c47-8b93-d545f67dc6bf.png)"
      ],
      "id": "ea7025ae"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f40b316"
      },
      "outputs": [],
      "source": [
        "class RefinementNetwork(nn.Module):\n",
        "    def __init__(self, in_channels=64, out_channels=3):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.Sequential(\n",
        "            # TODO\n",
        "        )\n",
        "    \n",
        "    def forward(self, input: torch.Tensor):\n",
        "        return self.blocks(input)\n",
        "\n",
        "test_input = torch.randn(2, 64, 128, 128)\n",
        "rn = RefinementNetwork()\n",
        "assert list(rn(test_input).shape) == [2, 3, 128, 128], rn(test_input).shape"
      ],
      "id": "3f40b316"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4061ce38"
      },
      "source": [
        "## Auxiliary network"
      ],
      "id": "4061ce38"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dc2b81e"
      },
      "outputs": [],
      "source": [
        "class VGG19(nn.Module):\n",
        "    def __init__(self, requires_grad=False):\n",
        "        super().__init__()\n",
        "        vgg_pretrained_features = torchvision.models.vgg19(\n",
        "            pretrained=True\n",
        "        ).features\n",
        "        self.slice1 = torch.nn.Sequential()\n",
        "        self.slice2 = torch.nn.Sequential()\n",
        "        self.slice3 = torch.nn.Sequential()\n",
        "        self.slice4 = torch.nn.Sequential()\n",
        "        self.slice5 = torch.nn.Sequential()\n",
        "        for x in range(2):\n",
        "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(2, 7):\n",
        "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(7, 12):\n",
        "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(12, 21):\n",
        "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(21, 30):\n",
        "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
        "        if not requires_grad:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Normalize the image so that it is in the appropriate range\n",
        "        h_relu1 = self.slice1(X)\n",
        "        h_relu2 = self.slice2(h_relu1)\n",
        "        h_relu3 = self.slice3(h_relu2)\n",
        "        h_relu4 = self.slice4(h_relu3)\n",
        "        h_relu5 = self.slice5(h_relu4)\n",
        "        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\n",
        "        return out"
      ],
      "id": "3dc2b81e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ecc0036"
      },
      "source": [
        "---"
      ],
      "id": "7ecc0036"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb955b50"
      },
      "source": [
        "# Criterions & Metrics"
      ],
      "id": "fb955b50"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad4a8159"
      },
      "outputs": [],
      "source": [
        "class PerceptualLoss(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Set to false so that this part of the network is frozen\n",
        "        self.model = VGG19(requires_grad=False)\n",
        "        self.criterion = nn.L1Loss()\n",
        "        self.weights = [1.0 / 32, 1.0 / 16, 1.0 / 8, 1.0 / 4, 1.0]\n",
        "\n",
        "    def forward(self, pred_img, gt_img):\n",
        "        gt_fs = self.model(gt_img)\n",
        "        pred_fs = self.model(pred_img)\n",
        "\n",
        "        # Collect the losses at multiple layers (need unsqueeze in\n",
        "        # order to concatenate these together)\n",
        "        loss = 0\n",
        "        for i in range(0, len(gt_fs)):\n",
        "            loss += self.weights[i] * self.criterion(pred_fs[i], gt_fs[i])\n",
        "\n",
        "        return loss\n"
      ],
      "id": "ad4a8159"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81835854"
      },
      "outputs": [],
      "source": [
        "def psnr(predicted_image, target_image):\n",
        "    batch_size = predicted_image.size(0)\n",
        "    mse_err = (\n",
        "        (predicted_image - target_image)\n",
        "        .pow(2).sum(dim=1)\n",
        "        .view(batch_size, -1).mean(dim=1)\n",
        "    )\n",
        "\n",
        "    psnr = 10 * (1 / mse_err).log10()\n",
        "    return psnr.mean()"
      ],
      "id": "81835854"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3e7d6f0"
      },
      "source": [
        "---"
      ],
      "id": "d3e7d6f0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "553d5034"
      },
      "source": [
        "# Point Cloud Renderer\n",
        "\n",
        "`Differential Rasterization` is a key component of our system. We will use the algorithm already implemented in `pytorch3d`.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/20357655/145715968-94abbe1a-8d14-4c20-98c4-61afd9161ada.png)\n",
        "\n",
        "For more details read (3.2) https://arxiv.org/pdf/1912.08804.pdf"
      ],
      "id": "553d5034"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73f59d89"
      },
      "outputs": [],
      "source": [
        "class PointsRasterizerWithBlending(nn.Module):\n",
        "    \"\"\"\n",
        "    Rasterizes a set of points using a differentiable renderer. \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, radius=1.5, image_size=256, points_per_pixel=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.radius = radius\n",
        "        self.image_size = image_size\n",
        "        self.points_per_pixel = points_per_pixel\n",
        "        \n",
        "        self.rad_pow = 2\n",
        "        self.tau = 1.0\n",
        "\n",
        "    def forward(self, point_cloud, spatial_features):\n",
        "        batch_size = spatial_features.size(0)\n",
        "\n",
        "        # Make sure these have been arranged in the same way\n",
        "        assert point_cloud.size(2) == 3\n",
        "        assert point_cloud.size(1) == spatial_features.size(2)\n",
        "\n",
        "        point_cloud[:, :, 1] = -point_cloud[:, :, 1]\n",
        "        point_cloud[:, :, 0] = -point_cloud[:, :, 0]\n",
        "\n",
        "        radius = float(self.radius) / float(self.image_size) * 2.0\n",
        "\n",
        "        point_cloud = Pointclouds(points=point_cloud, features=spatial_features.permute(0, 2, 1))\n",
        "        points_idx, _, dist = rasterize_points(\n",
        "            point_cloud, self.image_size, radius, self.points_per_pixel\n",
        "        )\n",
        "\n",
        "        dist = dist / pow(radius, self.rad_pow)\n",
        "\n",
        "        alphas = (\n",
        "            (1 - dist.clamp(max=1, min=1e-3).pow(0.5))\n",
        "            .pow(self.tau)\n",
        "            .permute(0, 3, 1, 2)\n",
        "        )\n",
        "    \n",
        "        transformed_src_alphas = compositing.alpha_composite(\n",
        "            points_idx.permute(0, 3, 1, 2).long(),\n",
        "            alphas,\n",
        "            point_cloud.features_packed().permute(1, 0),\n",
        "        )\n",
        "\n",
        "        return transformed_src_alphas"
      ],
      "id": "73f59d89"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8303a60"
      },
      "source": [
        "And `PointsManipulator` do the following steps:\n",
        "\n",
        "    1) Create virtual image place in [normalized coordinate](https://pytorch3d.org/docs/cameras)\n",
        "    2) Move camera according to `regressed depth`\n",
        "    3) Rotate points according to target camera paramers\n",
        "    4) And finally render them with help of `PointsRasterizerWithBlending`"
      ],
      "id": "b8303a60"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10446563"
      },
      "outputs": [],
      "source": [
        "class PointsManipulator(nn.Module):\n",
        "    EPS = 1e-5\n",
        "\n",
        "    def __init__(self, image_size):\n",
        "        super().__init__()\n",
        "        # Assume that image plane is square\n",
        "\n",
        "        self.splatter = PointsRasterizerWithBlending(\n",
        "            radius=1.0,\n",
        "            image_size=image_size,\n",
        "            points_per_pixel=128,\n",
        "        )\n",
        "\n",
        "        xs = torch.linspace(0, image_size - 1, image_size) / \\\n",
        "            float(image_size - 1) * 2 - 1\n",
        "        ys = torch.linspace(0, image_size - 1, image_size) / \\\n",
        "            float(image_size - 1) * 2 - 1\n",
        "\n",
        "        xs = xs.view(1, 1, 1, image_size).repeat(1, 1, image_size, 1)\n",
        "        ys = ys.view(1, 1, image_size, 1).repeat(1, 1, 1, image_size)\n",
        "\n",
        "        xyzs = torch.cat(\n",
        "            (xs, -ys, -torch.ones(xs.size()), torch.ones(xs.size())), 1\n",
        "        ).view(1, 4, -1)\n",
        "\n",
        "        self.register_buffer(\"xyzs\", xyzs)\n",
        "\n",
        "    def project_pts(self, depth, K, K_inv, RT_cam1, RTinv_cam1, RT_cam2, RTinv_cam2):\n",
        "        # Project the world points into the new view\n",
        "        projected_coors = self.xyzs * depth\n",
        "        projected_coors[:, -1, :] = 1\n",
        "\n",
        "        # Transform into camera coordinate of the first view\n",
        "        cam1_X = K_inv.bmm(projected_coors)\n",
        "\n",
        "        # Transform into world coordinates\n",
        "        RT = RT_cam2.bmm(RTinv_cam1)\n",
        "\n",
        "        wrld_X = RT.bmm(cam1_X)\n",
        "\n",
        "        # And intrinsics\n",
        "        xy_proj = K.bmm(wrld_X)\n",
        "\n",
        "        # And finally we project to get the final result\n",
        "        mask = (xy_proj[:, 2:3, :].abs() < self.EPS).detach()\n",
        "\n",
        "        # Remove invalid zs that cause nans\n",
        "        zs = xy_proj[:, 2:3, :]\n",
        "        zs[mask] = self.EPS\n",
        "\n",
        "        sampler = torch.cat((xy_proj[:, 0:2, :] / -zs, xy_proj[:, 2:3, :]), 1)\n",
        "        sampler[mask.repeat(1, 3, 1)] = -10\n",
        "        # Flip the ys\n",
        "        sampler = sampler * torch.Tensor([1, -1, -1]).unsqueeze(0).unsqueeze(\n",
        "            2\n",
        "        ).to(sampler.device)\n",
        "\n",
        "        return sampler\n",
        "\n",
        "    def forward_justpts(\n",
        "        self,\n",
        "        spatial_features, depth,\n",
        "        K, K_inv, RT_cam1, RTinv_cam1, RT_cam2, RTinv_cam2\n",
        "    ):\n",
        "        # Now project these points into a new view\n",
        "        batch_size, c, w, h = spatial_features.size()\n",
        "\n",
        "        if len(depth.size()) > 3:\n",
        "            # reshape into the right positioning\n",
        "            depth = depth.view(batch_size, 1, -1)\n",
        "            spatial_features = spatial_features.view(batch_size, c, -1)\n",
        "\n",
        "        pointcloud = self.project_pts(\n",
        "            depth, K, K_inv, RT_cam1, RTinv_cam1, RT_cam2, RTinv_cam2\n",
        "        )\n",
        "        pointcloud = pointcloud.permute(0, 2, 1).contiguous()\n",
        "        result = self.splatter(pointcloud, spatial_features)\n",
        "\n",
        "        return result"
      ],
      "id": "10446563"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0641cd48"
      },
      "source": [
        "---"
      ],
      "id": "0641cd48"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e5ab269"
      },
      "source": [
        "# All together"
      ],
      "id": "0e5ab269"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae8be184"
      },
      "outputs": [],
      "source": [
        "class ViewSynthesisModel(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.spatial_feature_predictor = SpatialFeatureNetwork()\n",
        "        self.depth_regressor = Unet(channels_in=3, channels_out=1)\n",
        "        self.point_cloud_renderer = PointsManipulator(image_size=256)\n",
        "        self.refinement_network = RefinementNetwork()\n",
        "\n",
        "        # Special constant for KITTI dataset\n",
        "        self.z_min = 1.0\n",
        "        self.z_max = 50.0\n",
        "\n",
        "    def forward(\n",
        "        self, \n",
        "        #TODO\n",
        "    ):\n",
        "        # TODO\n",
        "        # 1) Predict spatial feature for source image\n",
        "        # 2) Predict depth for source image (dont forget to renormalize depth with z_min/z_max)\n",
        "        # 3) Generate new features with `point_cloud_renderer`\n",
        "        # 4) And finnaly apply `refinement_network` to obtain new image\n",
        "        # 5) return new image, and depth of source image\n",
        "        return generated_image, regressed_depth"
      ],
      "id": "ae8be184"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3978e549"
      },
      "source": [
        "---"
      ],
      "id": "3978e549"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b553c72"
      },
      "source": [
        "# Training\n",
        "\n",
        "In order for the work to be accepted, you must achieve a quality of ~0.5 (validation loss value) and visualize several samples as in the example"
      ],
      "id": "3b553c72"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc7f94ea"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')\n",
        "\n",
        "model = ViewSynthesisModel().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
        "histoty = defaultdict(list)\n",
        "\n",
        "l1_criterion = nn.L1Loss()\n",
        "perceptual_criterion = PerceptualLoss().to(device)"
      ],
      "id": "cc7f94ea"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45f5a34e"
      },
      "outputs": [],
      "source": [
        "indexes = torch.randperm(len(dataset))\n",
        "\n",
        "train_indexes = indexes[:40]\n",
        "validation_indexes = indexes[:40]\n",
        "\n",
        "train_dataset = Subset(dataset, train_indexes)\n",
        "validation_dataset = Subset(dataset, validation_indexes)\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "val_batch_size = 4\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, batch_size=batch_size, num_workers=2,\n",
        "    shuffle=True, drop_last=True, pin_memory=True\n",
        ")\n",
        "validation_dataloder = DataLoader(\n",
        "    validation_dataset, batch_size=val_batch_size, num_workers=2,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "id": "45f5a34e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c2486550",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for i, batch in tqdm(enumerate(train_dataloader, 2), total=len(train_dataloader)):\n",
        "        source_image = batch[\"images\"][0].to(device)\n",
        "        target_image = batch[\"images\"][-1].to(device)\n",
        "\n",
        "        # TODO\n",
        "        generated_image, regressed_depth = model(...)\n",
        "\n",
        "        loss = l1_criterion(generated_image, target_image) \\\n",
        "            + 10 * perceptual_criterion(\n",
        "            renormalize_image(generated_image),\n",
        "            renormalize_image(target_image)\n",
        "        )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        histoty['train_loss'].append(loss.item())\n",
        "    \n",
        "    for i, batch in tqdm(enumerate(validation_dataloder), total=len(validation_dataloder)):\n",
        "        source_image = batch[\"images\"][0].to(device)\n",
        "        target_image = batch[\"images\"][-1].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # TODO\n",
        "            generated_image, regressed_depth = model(...)\n",
        "\n",
        "        loss = l1_criterion(generated_image, target_image) \\\n",
        "            + 10 * perceptual_criterion(\n",
        "            renormalize_image(generated_image),\n",
        "            renormalize_image(target_image)\n",
        "        )\n",
        "\n",
        "        histoty['validation_loss'].append(loss.item())\n",
        "    \n",
        "        \n",
        "    clear_output()\n",
        "    fig = plt.figure(figsize=(30, 15), dpi=80)\n",
        "    ax1 = plt.subplot(3, 3, 1)\n",
        "    ax1.plot(histoty['train_loss'], label='Train')\n",
        "    ax1.set_xlabel('Iterations', fontsize=20)\n",
        "    ax1.set_ylabel(r'${L_1} + Perceptual$', fontsize=20)\n",
        "    ax1.legend()\n",
        "    ax1.grid()\n",
        "    \n",
        "    ax2 = plt.subplot(3, 3, 4)\n",
        "    ax2.plot(histoty['validation_loss'], label='Validation')\n",
        "    ax2.set_xlabel('Iterations', fontsize=20)\n",
        "    ax2.set_ylabel(r'${L_1} + Perceptual$', fontsize=20)\n",
        "    ax2.legend()\n",
        "    ax2.grid()\n",
        "\n",
        "     for index, image in zip(\n",
        "        (2, 3, 5, 6),\n",
        "        (source_image, target_image, generated_image, regressed_depth.repeat(1, 3, 1,1))\n",
        "    ):\n",
        "        ax = plt.subplot(3, 3, index)\n",
        "        if index == 6:\n",
        "            im = ax.imshow(image.detach().cpu()[0].permute(1, 2, 0)[...,0])\n",
        "        else:\n",
        "            im = ax.imshow(renormalize_image(image.detach().cpu()[0]).permute(1, 2, 0))\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.show()"
      ],
      "id": "c2486550"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8559f1ea"
      },
      "source": [
        "# Visualize\n",
        "\n",
        "Goes along depth and generate new views"
      ],
      "id": "8559f1ea"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c941802a"
      },
      "outputs": [],
      "source": [
        "RTs = []\n",
        "for i in torch.linspace(0, 0.5, 40):\n",
        "    current_RT = torch.eye(4).unsqueeze(0)\n",
        "    current_RT[:, 2, 3] = i\n",
        "    RTs.append(current_RT.to(device))\n",
        "identity_matrx = torch.eye(4).unsqueeze(0).to(device)"
      ],
      "id": "c941802a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d775b314"
      },
      "outputs": [],
      "source": [
        "random_instance_index = 12\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    images, cameras = validation_dataset[random_instance_index].values()\n",
        "    # Input values\n",
        "    input_img = images[0][None].cuda()\n",
        "\n",
        "    # Camera parameters\n",
        "    K = torch.from_numpy(cameras[0][\"K\"])[None].to(device)\n",
        "    K_inv = torch.from_numpy(cameras[0][\"Kinv\"])[None].to(device)\n",
        "    \n",
        "    spatial_features = model.spatial_feature_predictor(input_img)\n",
        "    regressed_depth = model.depth_regressor(input_img)\n",
        "    regressed_depth = (regressed_depth + 1.) / 2. # 0..1\n",
        "    regressed_depth = regressed_depth * (model.z_max - model.z_min) + model.z_min\n",
        "\n",
        "    new_images = []\n",
        "    for current_RT in RTs:\n",
        "        generated_features = model.point_cloud_renderer.forward_justpts(\n",
        "            spatial_features,\n",
        "            regressed_depth,\n",
        "            K,\n",
        "            K_inv,\n",
        "            identity_matrx,\n",
        "            identity_matrx,\n",
        "            current_RT,\n",
        "            None\n",
        "        )\n",
        "        generated_image = model.refinement_network(generated_features)\n",
        "        new_images.append(renormalize_image(generated_image.cpu()).clamp(0, 1).mul(255).to(torch.uint8))"
      ],
      "id": "d775b314"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "563bb460"
      },
      "outputs": [],
      "source": [
        "\n",
        "to_img = lambda x: x[0].permute(1,2,0).numpy()\n",
        "import cv2\n",
        "# import torchvision\n",
        "video_name = 'video.mp4'\n",
        "video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'mp4v'), 20, (256, 256))\n",
        "\n",
        "for frame in new_images:\n",
        "    video.write(to_img(frame))\n",
        "\n",
        "cv2.destroyAllWindows()\n",
        "video.release()\n",
        "# Alternatively\n",
        "# frames = torch.cat(new_images).permute(0, 2, 3, 1)\n",
        "# torchvision.io.write_video('video.mp4', frames, fps=20)\n"
      ],
      "id": "563bb460"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dvuk1AYqyWN"
      },
      "outputs": [],
      "source": [
        "plt.imshow(to_img(frame))"
      ],
      "id": "1dvuk1AYqyWN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "602c87f9"
      },
      "outputs": [],
      "source": [
        "HTML(\"\"\"\n",
        "    <video width=\"256\" alt=\"test\" controls>\n",
        "        <source src=\"video.mp4\" type=\"video/mp4\">\n",
        "    </video>\n",
        "\"\"\")"
      ],
      "id": "602c87f9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "822381d1"
      },
      "source": [
        "# Quality benchmark"
      ],
      "id": "822381d1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c7aa5f3"
      },
      "source": [
        "Provide three video with your results for indices  101, 147 and 253. \n",
        "Also, put the result validation quality for your benchmark. Upload your View Synthesis module on google drive or yandex disk. We are not encourage you to train model as long as possible, because understand the limitation of the resources on Colab, that is why those data can help us to grade your work.\n",
        "\n",
        "| PSNR  | LPIPS  | \n",
        "|---|---|\n",
        "|   |   |\n",
        "|   |   |\n",
        "|   |   |"
      ],
      "id": "1c7aa5f3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcc06bee"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "fcc06bee"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "hw4-part2_novel_view_synthesis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}